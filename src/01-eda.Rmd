---
title: "Exploring Data & Identifying Model Features"
author: "Paul Johnson"
date: '`r Sys.Date()`'
output: 
  html_document: 
    theme: lumen
    ig_asp: 0.8
    fig_width: 10
    out_width: "100%"
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  cache = FALSE,
  warning = FALSE,
  message = FALSE
  )

# import packages
library(dplyr)
library(ggplot2)
library(janitor)


# import train/test data and resamples
load(here::here("data", "vb_data.RData"))

# import custom functions
miceadds::source.all(here::here("src", "functions"))

# set ggplot theme
theme_set(theme_minimal(base_size = 14))

```

The first step is to carry out Exploratory Data Analysis (EDA), with the intention of gaining an understanding of how the data is structured, identifying any issues in the data that need to be resolved, and identifying and extracting any potential features for the model.

# Understanding the Structure of the Data


```{r glimpse}

# take a look at the training set
glimpse(train_df)

```

There are plenty of observations (11,165 rows), and we've got 15 columns, which means 1 target variable and 14 potential features, however, part of the earlier data processing step was selecting variables that could be relevant to predicting kills, so the 14 variables should be promising.

The majority of the variables are numeric and the rest are character variables. From an initial look at the numeric values, it doesn't look like there's a huge range and everything is more on less on the same scale, which is good news.

Several of the character variables look like they could be transformed to be more useful. For example, the rotation variable looks like it should probably be a numeric variable instead.

# Visualizing the Binary Target

The target variable is *evaluation_code*. Though in the raw data it isn't a binary variable, we are interested in just two of the variable's values (kills and errors) so it was transformed in the initial processing step. Kills are the positive event level (points scored by the team making the play), while errors are the negative event level.

```{r target-frequencies}

train_df %>%
  tabyl(evaluation_code)

```

The target classes are relatively imbalanced, which could be a problem. The classes are split \~70/30, with kills being the majority class. This means that our models are at risk of overprescribing kills in the predictions, because machine learning algorithms like to take shortcuts! If a model was to predict that every outcome will be a kill, it would be right \~70% of the time.

```{r target-plot}

train_df %>%
  ggplot(aes(evaluation_code)) +
  geom_bar(position = "dodge") +
  scale_fill_viridis_d() +
  labs(x = NULL, y = NULL, fill = NULL)

```

The bar plot probably does a better job of demonstrating how significant the imbalance is between kills and errors. It will be necessary to explore options for how to negate the negative effects of class imbalance.

# Feature Exploration & Extraction

## Skill Type

Skill type seems like one of the most likely variables to contain significant predictive power. The type of attack seems like it should contain information about the conditional probability of success.

```{r skill-type-frequencies}

train_df %>%
  tabyl(skill_type)

```

Just over 50% of the skill type's are head ball attacks, while the remaining 50% is mostly split between quick ball attacks, high ball attacks, and slide ball attacks. The distribution of the skill types seems pretty reasonable (despite the overwhelming dominance of head ball attacks), as it's only 'other attack' that is really poorly represented.

*I've created functions to plot the different charts to avoid repetition. The functions can be found in '~/src/functions'.*

```{r skill-type}

plot_bars(skill_type)

```

It looks like there's some variance in skill type split by the outcome.

```{r skill-type-props}

train_df %>%
  tabyl(skill_type, evaluation_code) %>%
  adorn_percentages()

```

It's a little easier to make sense of this by looking at the skill type percentages split by outcome. Given that kills and errors are split \~70/30, we can compare skill types that have higher or lower proportions.

-   Head ball attacks lead to \~2% higher than average kills, but that difference is potentially meaningful.
-   Slide ball attacks lead to a little bit less than 2% fewer kills (which was hard to infer from the plot).
-   Quick ball attacks result in \~6% more kills than average.
-   High ball attacks and other attacks tend to produce worse than average outcomes (I'm guessing that the other attacks come as a result of limited options available to the attacking team).

All this suggests there's definitely some predictive power in this feature.

## Skill Subtype

In addition to skill types, skill subtypes could also contain some useful variance and potentially some predictive power.

```{r skill-subtype-frequencies}

train_df %>%
  tabyl(skill_subtype)

```

The vast majority of the subtypes are hard spikes (\~88%), which limits how useful this feature can be.

```{r skill-subtype}

plot_bars(skill_subtype)

```

It's also pretty difficult to draw anything from the subtype plot, because of how overrepresented hard spikes are.

```{r skill-subtype-props}

train_df %>%
  tabyl(skill_subtype, evaluation_code) %>%
  adorn_percentages()

```

However, the trusty proportions can help us here! It looks like all three subtypes are pretty close to the average outcome proportions, but soft spikes/topspins do produce a \~2% lower than average kills.

I don't think subtypes will be entirely useless here, but the value in including it as a feature in our model is certainly limited. 

**Perhaps we can draw out more information if we interact the skill subtype with the skill type?**

## Start & End Locations

There are a total of seven variables that represent the location of points in the play. It's unlikely that we will be able to use all seven of these, so it's a case of identifying which will tell us the most.

There's a subset of four location variables that correspond to x and y coordinates, and the other three are locations binned in to zones. 

I think we gain more from the x, y coordinates than the zones and subzones. The more variance the better in this case.

```{r start-coordinates-plot}

plot_coordinates(start_coordinate_x, start_coordinate_y)

```

```{r end-coordinates-plot}

plot_coordinates(end_coordinate_x, end_coordinate_y)

```

It's very clear that the end coordinates vary meaningfully, however, it's less clear that there's anything important going on with the start coordinates. 

I think this is because there's simply less variance in the start coordinates, so it's harder to draw out information from it.

## Blockers

```{r blockers}

train_df %>%
  tabyl(blockers)

train_df %>%
  tabyl(blockers, evaluation_code) %>%
  adorn_percentages()

plot_bars(as.factor(blockers))

```

There's plenty of variance across different numbers of blockers, which isn't especially surprising. 

## Team/Opponent Setter Position

```{r setter-positions}

train_df %>%
  tabyl(team_setter_position)

train_df %>%
  tabyl(team_setter_position, evaluation_code) %>%
  adorn_percentages()

train_df %>%
  tabyl(opponent_setter_position, evaluation_code) %>%
  adorn_percentages()

plot_bars(as.factor(team_setter_position))

plot_bars(as.factor(opponent_setter_position))

```

There isn't tons of variance across different values of team and opponent setter position, though it's possible that these small differences matter. There appears to be slightly more variance across opponent setter position, though it's not clear if this is by chance.

**Do the two have meaningful interactions?**

## Rotation

```{r rotation}

train_df %>%
  tabyl(rotation)

train_df %>%
  tabyl(rotation, evaluation_code) %>%
  adorn_percentages()

```
Rotation is just another representation of team setter position.

## Contact

```{r contact}

train_df %>%
  tabyl(contact)

```
There's virtually no variation across the three values of contact (1, 2, 3). Of 11,165 observations only 11 do not equal 3.
