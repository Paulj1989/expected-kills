---
title: "Exploring Data & Identifying Model Features"
author: "Paul Johnson"
date: '`r Sys.Date()`'
output: 
  html_document: 
    ig_asp: 0.8
    fig_width: 10
    out_width: 100%
editor_options: 
  chunk_output_type: inline
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  cache = TRUE,
  warning = FALSE,
  message = FALSE
  )

# import packages
library(dplyr)
library(ggplot2)
library(janitor)

# import custom functions
miceadds::source.all(here::here("src", "functions"))

# import train/test data and resamples
load(here::here("data", "vb_data.RData"))

# set ggplot theme
theme_set(theme_minimal(base_size = 14))

```

The first step in the machine learning process (and any statistical analysis project) is the Exploratory Data Analysis (EDA).

This is an important part of the process because otherwise you're going in blind. It's very difficult to know how to build a model to predict an outcome when you don't know what your data looks like, how your targets and features are distributed, and what issues exist in the data that will need to be resolved first.

Because some initial data processing has been carried out previously, the goal here is to identify features that appear to have some predictive power, whether they exist in the data already or have to be extracted from the data in its current form, and to understand what transformations will be needed in the preprocessing recipe to maximize the predictive capabilities of our eventual feature set.

# Understanding the Structure of the Data

It's important to understand what the dataset looks like in its full form. How many observations are there in the training set, and how many potential features do we have?

```{r glimpse}

# take a look at the training set
glimpse(train_df)

```

The *glimpse()* function is a really good starting point for understanding the data because it gives us all sorts of useful information at a glance.

There are plenty of observations (11,165 rows), and we've got 15 columns, which means 1 target variable and 14 potential features, however, part of the earlier data processing step was selecting variables that could be relevant to predicting kills, so the 14 variables should be promising.

The majority of the variables are numeric and the rest are character variables. From an initial look at the numeric values, it doesn't look like there's a huge range and everything is more on less on the same scale, which is good news.

Several of the character variables look like they could be transformed to be more useful. For example, the rotation variable looks like it should probably be a numeric variable instead.

# Visualizing the Binary Target

The target variable is *evaluation_code*. Though in the raw data it isn't a binary variable, we are interested in just two of the variable's values: kills and errors. Kills are the positive event level (points scored by the team making the play), while errors are the negative event level.

```{r target-frequencies}

train_df %>%
  tabyl(evaluation_code)

```

The target classes are relatively imbalanced, which could be a problem. The classes are split \~70/30, with kills being the majority class. This means that our models are at risk of overprescribing kills in the predictions, because machine learning algorithms like to take shortcuts! If a model was to predict that every outcome will be a kill, it would be right \~70% of the time.

```{r target-plot}

train_df %>%
  ggplot(aes(evaluation_code)) +
  geom_bar(position = "dodge") +
  scale_fill_viridis_d() +
  labs(x = NULL, y = NULL, fill = NULL)

```

The bar plot probably does a better job of demonstrating how significant the imbalance is between kills and errors. It will be necessary to explore options for how to negate the negative effects of class imbalance.

# Feature Exploration & Extraction

## Skill Type

Skill type seems like one of the most likely variables to contain significant predictive power. The type of attack seems like it should contain information about the conditional probability of success.

```{r skill-type-frequencies}

train_df %>%
  tabyl(skill_type)

```

Just over 50% of the skill type's are head ball attacks, while the remaining 50% is mostly split between quick ball attacks, high ball attacks, and slide ball attacks. The distribution of the skill types seems pretty reasonable (despite the overwhelming dominance of head ball attacks), as it's only 'other attack' that is really poorly represented.

```{r skill-type}

train_df %>%
  ggplot(aes(evaluation_code, fill = skill_type)) +
  geom_bar(position = "dodge") +
  scale_fill_viridis_d() +
  labs(x = NULL, y = NULL, fill = NULL)

```

It looks like there's some variance in skill type split by the outcome. Head ball attacks seem to have a pretty good chance of ending as a kill, and both quick ball attacks and slide ball attacks look like they're disproportionately likely to be kills.

```{r skill-type-props}

train_df %>%
  tabyl(skill_type, evaluation_code) %>%
  adorn_percentages()

```

It's a little easier to make sense of this by looking at the skill type percentages split by outcome. Given that kills and errors are split \~70/30, we can compare skill types that have higher or lower proportions.

The skill type plot made it look like head ball attacks probably disproportionately favor kills, and this does appear to be the case. The difference is relatively small, as head ball attacks lead to \~2% higher than average kills, but that difference is potentially quite meaningful.

Interestingly, it seems that the plot may be a little misleading (or I'm stupid) when it comes to slide ball attacks. It looks like they led to a higher proportion of kills, however it looks like they actually lead to a little bit less than 2% fewer kills). This is probably due to larger quick ball attacks bar next to the slide ball attacks bar giving me the wrong impression.

However, the good news is that quick ball attacks definitely lead to a higher proportion of kills, as they result in \~6% more kills than average. And in contrast, high ball attacks and other attacks tend to produce worse than average outcomes (I'm guessing that the other attacks come as a result of limited options available to the attacking team).

All this suggests there's definitely some predictive power in this feature.

## Skill Subtype

In addition to skill types, skill subtypes could also contain some useful variance and potentially some predictive power.

```{r skill-subtype-frequencies}

train_df %>%
  tabyl(skill_subtype)

```

The vast majority of the subtypes are hard spikes (\~88%), which limits how useful this feature can be.

```{r skill-subtype}

train_df %>%
  ggplot(aes(evaluation_code, fill = skill_subtype)) +
  geom_bar(position = "dodge") +
  scale_fill_viridis_d() +
  labs(x = NULL, y = NULL, fill = NULL)

```

It's also pretty difficult to draw anything from the subtype plot, because of how overrepresented hard spikes are.

```{r skill-subtype-props}

train_df %>%
  tabyl(skill_subtype, evaluation_code) %>%
  adorn_percentages()

```

However, the trusty proportions can help us here! It looks like all three subtypes are pretty close to the average outcome proportions, but soft spikes/topspins do produce a \~2% lower than average kills.

I don't think subtypes will be entirely useless here, but the value in including it as a feature in our model is certainly limited. Perhaps we can draw out more information if we interact the skill subtype with the skill type?

## Start & End Locations

There are a total of seven variables that represent the location of points in the play. It's unlikely that we will be able to use all seven of these, so it's a case of identifying which will tell us the most.

There's a subset of four location variables that correspond to x and y coordinates, and the other three are locations binned in to zones. 

I think we gain more from the x, y coordinates than the zones and subzones. The more variance the better in this case.

```{r start-coordinates-plot}

train_df %>%
  ggplot(aes(x = start_coordinate_x, y = start_coordinate_y)) +
  geom_hex() +
  scale_fill_viridis_c() +
  labs(fill = NULL) +
  facet_wrap(~evaluation_code)

```

```{r end-coordinates-plot}

train_df %>%
  ggplot(aes(x = end_coordinate_x, y = end_coordinate_y)) +
  geom_hex() +
  scale_fill_viridis_c() +
  labs(fill = NULL) +
  facet_wrap(~evaluation_code)

```

It's very clear that the end coordinates vary meaningfully, however, it's less clear that there's anything important going on with the start coordinates. 

I think this is because there's simply less variance in the start coordinates, so it's harder to draw out information from it.

## Blockers

```{r blockers}

train_df %>%
  tabyl(blockers)

train_df %>%
  tabyl(blockers, evaluation_code) %>%
  adorn_percentages()

train_df %>%
  ggplot(aes(evaluation_code, fill = as.factor(blockers))) +
  geom_bar(position = "dodge") +
  scale_fill_viridis_d(direction = -1) +
  labs(x = NULL, y = NULL, fill = NULL)

```

There's plenty of variance across different numbers of blockers, which isn't especially surprising.

## Team/Opponent Setter Position

```{r setter-positions}

train_df %>%
  tabyl(team_setter_position)

train_df %>%
  tabyl(team_setter_position, evaluation_code) %>%
  adorn_percentages()

train_df %>%
  tabyl(opponent_setter_position, evaluation_code) %>%
  adorn_percentages()

train_df %>%
  ggplot(aes(evaluation_code, fill = as.factor(team_setter_position))) +
  geom_bar(position = "dodge") +
  scale_fill_viridis_d() +
  labs(x = NULL, y = NULL, fill = NULL)

train_df %>%
  ggplot(aes(evaluation_code, fill = as.factor(opponent_setter_position))) +
  geom_bar(position = "dodge") +
  scale_fill_viridis_d() +
  labs(x = NULL, y = NULL, fill = NULL)
```

There is greater variance with opponent setter position than team setter position. 

Do the two interact meaningfully?

## Rotation

```{r rotation}

train_df %>%
  tabyl(rotation)

train_df %>%
  tabyl(rotation, evaluation_code) %>%
  adorn_percentages()

```

I suspect that the small amount of variation we see across the six rotations is probably random variation.

## Contact

```{r contact}

train_df %>%
  tabyl(contact)

```

There's only eleven that are 1 or 2, everything else is 3. 
