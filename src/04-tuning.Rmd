---
title: "Optimizing & Testing the Best Performing Models"
author: "Paul Johnson"
date: '`r Sys.Date()`'
output: 
  html_document: 
    theme: lumen
    ig_asp: 0.8
    fig_width: 10
    out_width: "100%"
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(
  echo = TRUE,
  cache = FALSE,
  warning = FALSE,
  message = FALSE
  )

# import packages
library(dplyr)
library(ggplot2)
library(tidymodels)
library(yardstick)
library(dials)

# import train/test data, resamples, recipe, and metrics
load(here::here("data", "vb_data.RData"))
load(here::here("data", "preprocessed.RData"))

# import custom functions
miceadds::source.all(here::here("src", "functions"))

# set ggplot theme
theme_set(theme_minimal())

# specify that tidymodels output needs to work on a dark theme
options(tidymodels.dark = TRUE)

# specify parallelization for faster computation
# cores <- parallel::detectCores() - 1
# cl <- parallel::makeCluster(cores)
# doParallel::registerDoParallel(cl)

# unregister cluster
# parallel::stopCluster(cl)

```

Having trained the basic candidate models in the [previous notebook](./src/03-training.Rmd) we can select a subset of models that performed well according to our evaluation metrics. This subset of models can then be optimized to the data by tuning their hyperparameters. This should improve the performance of all of the models, and the best performing model after this process can be selected as the final model.

# Hyperparameter Optimization

Given that the Random Forest and the XGBoost have clearly outperformed their competitors but are pretty much neck-and-neck, we can go a little further and optimize the performance of these models by tuning the hyperparameters for this classification problem.

While the Random Forest ended up performing slightly better on three out of fours of the evaluation metrics, the XGBoost was actually performing better on the primary metric we're using, ROC-AUC. The XGBoost also has many hyperparameters that can be tuned to improve performance, so is perhaps more likely to see significant improvements in this optimization stage.

<!-- We will also include the K-Nearest Neighbours (KNN) model and the Neural Network in this stage. The KNN performed pretty well and serves as a high baseline to compare the Random Forest and XGBoost models, and while the Neural Network didn't perform particularly well, it should see significant improvements (perhaps even more so than the XGBoost) when tuned. While it's unlikely to come out on top in the end, it will be interesting to include it, so we are doing so! -->

## Define Models for Tuning

```{r models}

# random forest
rf_mod <-
  rand_forest(
    trees = 1000,
    min_n = tune(),
    mtry = tune()) %>%
  set_mode("classification") %>%
  set_engine("ranger")

rf_params <- rf_mod %>%
  extract_parameter_set_dials() %>%
  update(mtry = finalize(mtry(), train_df))
 
# xgboost
xgb_mod <-
  boost_tree(
    trees = 1000,
    tree_depth = tune(),
    min_n = tune(),
    loss_reduction = tune(),
    sample_size = tune(),
    mtry = tune(),
    learn_rate = tune()
  ) %>%
  set_mode("classification") %>%
  set_engine("xgboost")

xgb_params <- xgb_mod %>% 
  extract_parameter_set_dials() %>%
  update(sample_size = sample_prop(),
       mtry = finalize(mtry(), train_df))

```

## Set Worklow Pipeline

```{r workflow}

model_pipeline <- 
   workflow_set(
      preproc = list(model_rec),
      models = list(
        rf = rf_mod,
        xgb = xgb_mod
        )
   ) %>%
  mutate(wflow_id = gsub("(recipe_)", "", wflow_id)) %>%
  option_add(param_info = rf_params, id = "rf") %>%
  option_add(param_info = xgb_params, id = "xgb")

model_pipeline %>%
  extract_workflow(id = "xgb")

```

## Tune Model Hyperparameters

First we need to specify the grid search process.

```{r grid}

# specify grid search process
grid_ctrl <- tune::control_grid(
  parallel_over = "resamples",
  event_level = "second",
  verbose = FALSE,
  save_pred = TRUE,
  save_workflow = TRUE,
)

```

Then we are ready to tune the models.

```{r tune, cache = TRUE, results = 'hide'}

# tune models
tune_models <-
  model_pipeline %>% 
  # map across all preprocessing steps and models in workflow set
   workflow_map(
     "tune_grid",
     # set seed for reproducibility
     seed = 456,
     # identify resamples
     resamples = train_folds,
     # specify grid size
     grid = 10,
     # grid search specs
     control = grid_ctrl,
     # metrics for evaluating performance
     metrics = eval_metrics,
     # log results throughout training process
     verbose = TRUE)

```

```{r save-model}

readr::write_rds(tune_models, here::here("data", "tuned_models.rds"))

```

## Evaluate Performance

```{r evaluation}

tune_models %>%
  evaluate_performance("roc_auc", select_best = TRUE)

tune_models %>%
  evaluate_performance("accuracy", select_best = TRUE)

tune_models %>%
  evaluate_performance("f_meas", select_best = TRUE)

tune_models %>%
  evaluate_performance("j_index", select_best = TRUE)

# visualize performance across all metrics
autoplot(tune_models) +
    scale_color_viridis_d()

```
